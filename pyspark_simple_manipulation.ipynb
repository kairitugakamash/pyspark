{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df5212e-6663-44e9-ae2a-1507f9b0f5da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n  Using cached pyspark-3.5.4-py2.py3-none-any.whl\r\nCollecting py4j==0.10.9.7\r\n  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\r\nInstalling collected packages: py4j, pyspark\r\nSuccessfully installed py4j-0.10.9.7 pyspark-3.5.4\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-333b67dc-8755-48b6-abf5-9a72f385db21/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nCollecting wget\r\n  Using cached wget-3.2-py3-none-any.whl\r\nInstalling collected packages: wget\r\nSuccessfully installed wget-3.2\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-333b67dc-8755-48b6-abf5-9a72f385db21/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (1.4.2)\r\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas) (2.8.2)\r\nRequirement already satisfied: numpy>=1.18.5 in /databricks/python3/lib/python3.9/site-packages (from pandas) (1.21.5)\r\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas) (2021.3)\r\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n\u001B[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\r\nYou should consider upgrading via the '/local_disk0/.ephemeral_nfs/envs/pythonEnv-333b67dc-8755-48b6-abf5-9a72f385db21/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark\n",
    "# !pip install wget\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17a9b908-3a39-4c41-9694-be391e429cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5bff586-012f-4bef-bcb3-bd639acb9ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6daf250b-603c-4201-b55b-9caf2e61e046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Instantiate spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5df7267-0299-459c-8d53-bf98852708d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1081780367670850#setting/sparkui/0101-182501-wv7pr1nd/driver-7050520965767902364\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1081780367670850#setting/sparkui/0101-182501-wv7pr1nd/driver-7050520965767902364\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiate spark session\n",
    "spark = SparkSession.builder.appName(\"Kairitu_pyspark\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5d9854-230c-42d7-8528-e17cb8c9478a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: ['Builder',\n '__annotations__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__enter__',\n '__eq__',\n '__exit__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_activeSession',\n '_conf',\n '_convert_from_pandas',\n '_createFromLocal',\n '_createFromLocalTrusted',\n '_createFromRDD',\n '_create_dataframe',\n '_create_from_pandas_with_arrow',\n '_create_rdd_from_local_trusted',\n '_create_shell_session',\n '_getActiveSessionOrCreate',\n '_get_numpy_record_dtype',\n '_inferSchema',\n '_inferSchemaFromList',\n '_instantiatedSession',\n '_jconf',\n '_jsc',\n '_jsparkSession',\n '_jvm',\n '_repr_html_',\n '_sc',\n '_wrap_data_schema',\n '_write_to_trusted_path',\n 'builder',\n 'catalog',\n 'conf',\n 'createDataFrame',\n 'getActiveSession',\n 'newSession',\n 'range',\n 'read',\n 'readStream',\n 'sparkContext',\n 'sql',\n 'stop',\n 'streams',\n 'table',\n 'udf',\n 'version']"
     ]
    }
   ],
   "source": [
    "dir(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c45e77-bcb2-4821-b8fb-1982f903a9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[11]: ['__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_jcatalog',\n '_jsparkSession',\n '_reset',\n '_sparkSession',\n 'cacheTable',\n 'clearCache',\n 'createExternalTable',\n 'createTable',\n 'currentCatalog',\n 'currentDatabase',\n 'databaseExists',\n 'dropGlobalTempView',\n 'dropTempView',\n 'functionExists',\n 'getDatabase',\n 'getFunction',\n 'getTable',\n 'isCached',\n 'listCatalogs',\n 'listColumns',\n 'listDatabases',\n 'listFunctions',\n 'listTables',\n 'recoverPartitions',\n 'refreshByPath',\n 'refreshTable',\n 'registerFunction',\n 'setCurrentCatalog',\n 'setCurrentDatabase',\n 'tableExists',\n 'uncacheTable']"
     ]
    }
   ],
   "source": [
    "dir(spark.catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "811361ca-ef7a-40d6-9dad-01c5e3da7304",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Catalog : spark_catalog\nList of catalogs : [CatalogMetadata(name='spark_catalog', description=None)]\nCurrent Database : default\nList Databases : [Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='dbfs:/user/hive/warehouse')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current Catalog : {spark.catalog.currentCatalog()}\")\n",
    "print(f\"List of catalogs : {spark.catalog.listCatalogs()}\")\n",
    "print(f\"Current Database : {spark.catalog.currentDatabase()}\")\n",
    "print(f\"List Databases : {spark.catalog.listDatabases()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cba61ff6-c44c-4396-8bae-c7e1efaa62a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simple dataset of employees and departments\n",
    "1. how to create a simple dataset\n",
    "2. how to rename columns, convert to lowercase ..\n",
    "3. how to perform join based on a common field in the two datasets\n",
    "4. Rearrange columns\n",
    "5. drop a column or columns\n",
    "5. how to query the dataset\n",
    "6. how to filter based on a given criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa9378c-b928-4514-b277-7a385e501f8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------------+------+\n|age|deptId|gender|         name|salary|\n+---+------+------+-------------+------+\n| 40|     1|     M| Hyukjin Kwon|    50|\n| 50|     1|     M|Takuya Ueshin|   100|\n| 60|     2|     F| Xinrong Meng|   150|\n| 20|     3|     M|  Haejoon Lee|   200|\n+---+------+------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# To create DataFrame using SparkSession\n",
    "people = spark.createDataFrame([\n",
    "    {\"deptId\": 1, \"age\": 40, \"name\": \"Hyukjin Kwon\", \"gender\": \"M\", \"salary\": 50},\n",
    "    {\"deptId\": 1, \"age\": 50, \"name\": \"Takuya Ueshin\", \"gender\": \"M\", \"salary\": 100},\n",
    "    {\"deptId\": 2, \"age\": 60, \"name\": \"Xinrong Meng\", \"gender\": \"F\", \"salary\": 150},\n",
    "    {\"deptId\": 3, \"age\": 20, \"name\": \"Haejoon Lee\", \"gender\": \"M\", \"salary\": 200}\n",
    "])\n",
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de7a7cdb-6cad-4f8c-85db-23bf0b0c6b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n|dept_name| id|\n+---------+---+\n|  PySpark|  1|\n|       ML|  2|\n|Spark SQL|  3|\n+---------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# To create DataFrame using SparkSession\n",
    "department = spark.createDataFrame([\n",
    "    {\"id\": 1, \"dept_name\": \"PySpark\"},\n",
    "    {\"id\": 2, \"dept_name\": \"ML\"},\n",
    "    {\"id\": 3, \"dept_name\": \"Spark SQL\"}\n",
    "])\n",
    "\n",
    "department.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f0f74d7-f0c7-4c2e-aaeb-777c9c057f71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Column names renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fd7a3da-aa7b-47d3-9009-361a4f7845e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: ['dept_name', 'dept_id']"
     ]
    }
   ],
   "source": [
    "department = department.withColumnRenamed(\"id\", \"dept_id\")\n",
    "department.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d52dff-56bc-4b16-a514-188a51ff3ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: ['age', 'dept_id', 'gender', 'name', 'salary']"
     ]
    }
   ],
   "source": [
    "# people.columns\n",
    "## Out[23]: ['age', 'deptId', 'gender', 'name', 'salary']\n",
    "\n",
    "# people=people.withColumnRenamed(\"deptId\", \"dept_id\")\n",
    "# people.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ac4854-10c2-45f6-b54d-7260034e104b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after renaming : ['age', 'deptid', 'gender', 'name', 'salary']\nafter renaming : ['dept_name', 'dept_id']\n"
     ]
    }
   ],
   "source": [
    "# convert all column names to lowercase\n",
    "for col in people.columns:\n",
    "    people = people.withColumnRenamed(col, col.lower())\n",
    "print(f\"after renaming : {people.columns}\")\n",
    "\n",
    "for col in department.columns:\n",
    "    department = department.withColumnRenamed(col, col.lower())\n",
    "print(f\"after renaming : {department.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b28eac2-52e9-4f76-833d-331ba40019d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# perform join based on a common field in the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f9ddc2-00b3-49a5-8636-24c064bec609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------------+------+---------+-------+\n|age|deptid|gender|         name|salary|dept_name|dept_id|\n+---+------+------+-------------+------+---------+-------+\n| 40|     1|     M| Hyukjin Kwon|    50|  PySpark|      1|\n| 50|     1|     M|Takuya Ueshin|   100|  PySpark|      1|\n| 60|     2|     F| Xinrong Meng|   150|       ML|      2|\n| 20|     3|     M|  Haejoon Lee|   200|Spark SQL|      3|\n+---+------+------+-------------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_ds = people.join(department, people.deptid == department.dept_id)\n",
    "joined_ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66aaea62-14de-4b59-8760-bf18bf1594ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Rearrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260efb1f-70c4-4e30-a2c4-39e202829dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: ['age', 'deptid', 'gender', 'name', 'salary', 'dept_name', 'dept_id']"
     ]
    }
   ],
   "source": [
    "joined_ds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46ae4840-c7e1-4a3a-8ee8-91570e4fbcf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+------+---------+-------+------+------+\n|     emp_name|age|gender|dept_name|dept_id|salary|deptid|\n+-------------+---+------+---------+-------+------+------+\n| Hyukjin Kwon| 40|     M|  PySpark|      1|    50|     1|\n|Takuya Ueshin| 50|     M|  PySpark|      1|   100|     1|\n| Xinrong Meng| 60|     F|       ML|      2|   150|     2|\n|  Haejoon Lee| 20|     M|Spark SQL|      3|   200|     3|\n+-------------+---+------+---------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_ds = joined_ds.select('name','age', 'gender', 'dept_name', 'dept_id', 'salary', 'deptid')\n",
    "joined_ds = joined_ds.withColumnRenamed('name', 'emp_name')\n",
    "joined_ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40575a11-47c0-43a3-8d3f-6f610480537d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# drop a column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09fc322d-41cf-4a9f-a510-1b10ed68e50c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0bf017-4072-44d4-a0f0-c7f5649809b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: ['emp_name', 'age', 'gender', 'dept_name', 'dept_id', 'salary']"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['deptid']\n",
    "\n",
    "drop_option1 = joined_ds.drop(*columns_to_drop)\n",
    "\n",
    "drop_option1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6040e228-fccc-4b62-aa59-66144eb3394b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: ['emp_name', 'age', 'gender', 'dept_name', 'dept_id', 'salary', 'deptid']"
     ]
    }
   ],
   "source": [
    "joined_ds.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e681df2-3688-454c-ad58-b45b87c7b516",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ff60df-6233-4498-b0ba-6ca12bf1d32d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[50]: ['emp_name', 'age', 'gender', 'dept_name', 'salary']"
     ]
    }
   ],
   "source": [
    "drop_list = ['dept_id', 'deptid']\n",
    "\n",
    "drop_option2 = joined_ds.select([column for column in joined_ds.columns if column not in drop_list])\n",
    "drop_option2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a1b2ca3-a1c2-4755-a7e6-68e9696eb48c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1dcc4bf-d6ac-4573-bcd8-cbc7fef1afc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+------+---------+-------+------+------+\n|     emp_name|age|gender|dept_name|dept_id|salary|deptid|\n+-------------+---+------+---------+-------+------+------+\n| Hyukjin Kwon| 40|     M|  PySpark|      1|    50|     1|\n|Takuya Ueshin| 50|     M|  PySpark|      1|   100|     1|\n| Xinrong Meng| 60|     F|       ML|      2|   150|     2|\n|  Haejoon Lee| 20|     M|Spark SQL|      3|   200|     3|\n+-------------+---+------+---------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c3a9f42-3c20-49c6-9738-bc021459fcb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+---------+-------+------+------+\n|    emp_name|age|gender|dept_name|dept_id|salary|deptid|\n+------------+---+------+---------+-------+------+------+\n|Hyukjin Kwon| 40|     M|  PySpark|      1|    50|     1|\n|Xinrong Meng| 60|     F|       ML|      2|   150|     2|\n| Haejoon Lee| 20|     M|Spark SQL|      3|   200|     3|\n+------------+---+------+---------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_ds.filter(people.age != 50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a225058c-ed1b-4dbb-bb93-8055874dedb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+------+---------+-------+------+------+\n|     emp_name|age|gender|dept_name|dept_id|salary|deptid|\n+-------------+---+------+---------+-------+------+------+\n| Hyukjin Kwon| 40|     M|  PySpark|      1|    50|     1|\n|Takuya Ueshin| 50|     M|  PySpark|      1|   100|     1|\n| Xinrong Meng| 60|     F|       ML|      2|   150|     2|\n|  Haejoon Lee| 20|     M|Spark SQL|      3|   200|     3|\n+-------------+---+------+---------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "joined_ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdaa70ed-6bf2-4826-9c6b-ee2ec12d5259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+------+---------+-------+------+------+\n|     emp_name|age|gender|dept_name|dept_id|salary|deptid|\n+-------------+---+------+---------+-------+------+------+\n| Hyukjin Kwon| 40|     M|  PySpark|      1|    50|     1|\n|Takuya Ueshin| 50|     M|  PySpark|      1|   100|     1|\n+-------------+---+------+---------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "fifty_or_pyspark = joined_ds.filter((joined_ds.age == 50) | (joined_ds.dept_name =='PySpark')\n",
    "    )\n",
    "fifty_or_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501a1aa0-b5dd-4549-9945-2f854d6188c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+------+---------+-------+------+------+\n|     emp_name|age|gender|dept_name|dept_id|salary|deptid|\n+-------------+---+------+---------+-------+------+------+\n|Takuya Ueshin| 50|     M|  PySpark|      1|   100|     1|\n+-------------+---+------+---------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "fifty_or_pyspark = joined_ds.filter((joined_ds.age == 50) & (joined_ds.dept_name =='PySpark')\n",
    "    )\n",
    "fifty_or_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77cc08fd-409f-4080-a9af-03481b302a73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+------+---------+-------+------+------+\n|    emp_name|age|gender|dept_name|dept_id|salary|deptid|\n+------------+---+------+---------+-------+------+------+\n|Xinrong Meng| 60|     F|       ML|      2|   150|     2|\n| Haejoon Lee| 20|     M|Spark SQL|      3|   200|     3|\n+------------+---+------+---------+-------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "fifty_or_pyspark = joined_ds.filter((joined_ds.age != 50) & (joined_ds.dept_name !='PySpark')\n",
    "    )\n",
    "fifty_or_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cf3ed49-2207-4ed0-a4fb-722fc81febfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Filter and join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99616e32-61be-47ac-9a3c-25a0d7132eb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------------+------+\n|age|deptid|gender|         name|salary|\n+---+------+------+-------------+------+\n| 40|     1|     M| Hyukjin Kwon|    50|\n| 50|     1|     M|Takuya Ueshin|   100|\n| 60|     2|     F| Xinrong Meng|   150|\n| 20|     3|     M|  Haejoon Lee|   200|\n+---+------+------+-------------+------+\n\nNone\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  PySpark|      1|\n|       ML|      2|\n|Spark SQL|      3|\n+---------+-------+\n\nNone\n"
     ]
    }
   ],
   "source": [
    "print(people.show())\n",
    "print(department.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b77e7d8-3bfa-444f-ba33-7435d9cea1fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+------+\n|dept_name|gender|lowest_pay|oldest|\n+---------+------+----------+------+\n|       ML|     F|       150|    60|\n|Spark SQL|     M|       200|    20|\n|  PySpark|     M|        50|    40|\n+---------+------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "people.filter(people.age != 50)\\\n",
    "    .join(department, people.deptid == department.dept_id)\\\n",
    "        .groupBy(department.dept_name, \"gender\")\\\n",
    "            .agg({\"age\": \"max\", \"salary\": \"min\",})\\\n",
    "                .withColumnRenamed('max(age)','oldest')\\\n",
    "                    .withColumnRenamed('min(salary)', 'lowest_pay')\\\n",
    "                        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ae857f-c460-437d-90c6-3d084c595c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----------+--------+\n|dept_name|gender|min(salary)|max(age)|\n+---------+------+-----------+--------+\n|       ML|     F|        150|      60|\n|Spark SQL|     M|        200|      20|\n|  PySpark|     M|         50|      40|\n+---------+------+-----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "filter_and_join = people.filter(people.age != 50)\\\n",
    "    .join(department, people.deptid == department.dept_id)\\\n",
    "        .groupBy(department.dept_name, \"gender\")\\\n",
    "            .agg({\"age\": \"max\", \"salary\": \"min\",})\n",
    "            \n",
    "filter_and_join.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
